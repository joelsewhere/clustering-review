{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a35774b3-aafa-4ad1-ab61-74ea308a3c67",
   "metadata": {
    "index": 0
   },
   "source": [
    "# Clustering Practice Notebook\n",
    "\n",
    "In the cell below we have generated 7 different datasets for you to cluster. This exercise is based on an [sklearn publication](https://scikit-learn.org/stable/auto_examples/cluster/plot_cluster_comparison.html) demonstrating how different clustering techniques behave. You are welcome to import additional clustering methods, and experiment with them if you'd like. In this notebook, we will focus on KMeans and Agglomerative clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15393a5c-756d-4fff-9e22-8c7be1c4f03e",
   "metadata": {
    "index": 1
   },
   "outputs": [],
   "source": [
    "# Run this cell unchanged\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn import cluster, datasets\n",
    "random_state = 170\n",
    "np.random.seed(0)\n",
    "n_samples = 1500\n",
    "data = []\n",
    "data.append(datasets.make_blobs(n_samples=n_samples, random_state=8)[0])\n",
    "data.append(datasets.make_blobs(n_samples=n_samples, random_state=random_state)[0])\n",
    "data.append(datasets.make_blobs(n_samples=n_samples,\n",
    "                             cluster_std=[1.0, 2.5, 0.5],\n",
    "                             random_state=random_state)[0])\n",
    "data.append(datasets.make_circles(n_samples=n_samples, factor=.5,\n",
    "                                      noise=.05)[0])\n",
    "data.append(datasets.make_moons(n_samples=n_samples, noise=.05)[0])\n",
    "data.append(np.random.rand(n_samples, 2))\n",
    "\n",
    "\n",
    "X, y = datasets.make_blobs(n_samples=n_samples, random_state=random_state)\n",
    "transformation = [[0.6, -0.6], [-0.4, 0.8]]\n",
    "X_aniso = np.dot(X, transformation)\n",
    "\n",
    "data.append(X_aniso)\n",
    "\n",
    "fig, axes = plt.subplots(2,4, figsize=(15,6))\n",
    "\n",
    "titles = ['Tight and far', 'Tight and close', 'Uneven variance', 'Circles',\n",
    "          'Moons', 'No Clusters', 'Non Circular']\n",
    "\n",
    "for idx in range(1,8):\n",
    "    ax = axes[(idx-1)//4, (idx-1)%4]\n",
    "    X = data[idx-1]\n",
    "    ax.scatter(X[:,0], X[:,1])\n",
    "    ax.set_title(titles[idx-1], fontsize=15)\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56c33b33-029b-441e-b49b-ed7ad7082441",
   "metadata": {
    "index": 2
   },
   "source": [
    "# Dataset # 1\n",
    "\n",
    "In the cell below, we visualize the first dataset and assigned the data to the variable `data_1`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "893246f3-1c9d-4abd-8c19-a11d3c931a15",
   "metadata": {
    "index": 3
   },
   "outputs": [],
   "source": [
    "# Run this cell unchanged\n",
    "data_1 = data[0]\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(data_1[:,0], data_1[:,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3195e5e0-5385-4e2e-ad25-6c4211eae910",
   "metadata": {
    "index": 4
   },
   "source": [
    "In the cell below, import the Kmeans clustering object from sklearn, and generate predictions for the dataset. Store the predictions in the variable `preds`.\n",
    "\n",
    ">Because clustering techniques are typically relient on distance metrics, you will also need to scale!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72632c5f-598d-4915-990a-3e977dab0def",
   "metadata": {
    "index": 5
   },
   "outputs": [],
   "source": [
    "# Import the necessary sklearn objects\n",
    "\n",
    "# Scale the data\n",
    "\n",
    "# Initialize a Kmeans object with a selected number of clusters\n",
    "\n",
    "# Fit the kmeans object to the data\n",
    "\n",
    "# Generate predictions for the data\n",
    "\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(data_1[:,0], data_1[:,1], c=preds);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57544587-f704-4c56-bba6-11274cfa2a44",
   "metadata": {
    "index": 7
   },
   "source": [
    "# Dataset #2\n",
    "\n",
    "In the cell below, we visualize the second dataset and assigned the data to the variable `data_2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ccb0cd-c524-4552-8932-57bb7afcf1b9",
   "metadata": {
    "index": 8
   },
   "outputs": [],
   "source": [
    "# Run this cell unchanged\n",
    "data_2 = data[1]\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(data_2[:,0], data_2[:,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12203317-1449-4dfb-a36f-899e857ebd36",
   "metadata": {
    "index": 9
   },
   "source": [
    "In the cell below, apply the kmeans algorithm to dataset 2, and assign your predictions to the variable `preds`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4b37487-8d62-4604-84bc-680d00070558",
   "metadata": {
    "index": 10
   },
   "outputs": [],
   "source": [
    "# Scale data\n",
    "\n",
    "# Create a kmeans object\n",
    "\n",
    "# Fit kmeans to the data\n",
    "\n",
    "# Generate predictions\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(data_2[:,0], data_2[:,1], c=preds);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2bc90e10-ed36-44b7-9bc6-9e69923ef657",
   "metadata": {
    "index": 12
   },
   "source": [
    "## Finding `n_clusters`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "742c0c5c-2eb2-4bb4-9b25-41c9c23073e5",
   "metadata": {
    "index": 13
   },
   "source": [
    "This is all fine an dandy, but what if we do not know the number of clusters?\n",
    "\n",
    "If we have reason to believe that the clusters of our data are structured similary to above (A sort of spherical or \"blob\" like shape), then the `silhouette score` often times does a good job of finding the optimal number of clusters.\n",
    "\n",
    "Let's try it out. In the cell below, import `silhouette_score` from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5b9529-7c59-48fd-8643-066fd63b12db",
   "metadata": {
    "index": 14
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "362783dd-68de-45cb-bd51-cc2e49697af6",
   "metadata": {
    "index": 16
   },
   "source": [
    "Now we will use `np.random` to select a random number of clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c0c2031-1adf-4b57-8bb0-ad5215a8818c",
   "metadata": {
    "index": 17
   },
   "outputs": [],
   "source": [
    "# Run this cell unchanged\n",
    "\n",
    "# Randomly select an integer between 2 and 10\n",
    "n_clusters = np.random.choice(range(2, 11))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e4cb598-7e73-44a8-8661-f65fd3c9b9ab",
   "metadata": {
    "index": 18
   },
   "source": [
    "Next, we will create a dataset of clusters setting the number of clusters to the randomly selected integer from the cell above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab097cbc-dd44-49d3-94c8-b39f31682879",
   "metadata": {
    "index": 19
   },
   "outputs": [],
   "source": [
    "# Run this cell unchanged\n",
    "\n",
    "# Create the dataset with an unknown # of clusters\n",
    "unknown = datasets.make_blobs(n_samples=n_samples, centers=n_clusters)[0]\n",
    "\n",
    "# Scale the data for clustering with kmeans\n",
    "unknown = StandardScaler().fit_transform(unknown)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63011a2f-3f13-484f-8b8e-9dec7dce10b4",
   "metadata": {
    "index": 20
   },
   "source": [
    "Now we will loop over a range of numbers. For this experiment we will loop over 2 to 20 with the assumption that we have absolutely no idea how many clusters are in the data. \n",
    "\n",
    "For each iteration, we will:\n",
    "1. Fit a kmeans model to the variable `unknown` with our guess for the number of clusters\n",
    "1. Generate predictions for the data\n",
    "1. Pass the dataset and the predictions into `silhouette_score`\n",
    "1. Append those scores to a list\n",
    "1. Plot our scores for each guess.\n",
    "\n",
    "We will also plot the elbow plot for [kmean inertia](https://stats.stackexchange.com/a/78413) which is another way of selecting n_clusters for the kmeans algorithm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c5ab36c-0f1a-4f76-a9d7-60b6cbb2c481",
   "metadata": {
    "index": 21
   },
   "outputs": [],
   "source": [
    "# Run this cell unchanged\n",
    "\n",
    "# Run this cell unchanged\n",
    "fig, axes = plt.subplots(1, 3, figsize=(15,6))\n",
    "\n",
    "scores = []\n",
    "inertias = []\n",
    "for k in range(2,20):\n",
    "    # Fit a kmeans with a guess at the number of clusters\n",
    "    kmeans = KMeans(n_clusters=k)\n",
    "    kmeans.fit(unknown)\n",
    "    # Generate predictions\n",
    "    preds = kmeans.predict(unknown)\n",
    "    # Append our scores\n",
    "    score = silhouette_score(unknown, preds)\n",
    "    scores.append(score)\n",
    "    inertias.append(kmeans.inertia_)\n",
    "\n",
    "    \n",
    "#\n",
    "# Plot the silhouette scores\n",
    "best_cluster = sorted(list(enumerate(scores)), key=lambda x: x[1])[-1][0] + 2\n",
    "axes[0].plot(range(2,20), scores)\n",
    "axes[0].set_title('Silhouette Scores')\n",
    "axes[0].vlines(best_cluster, min(scores), max(scores), linestyle='--', label='Best n_clusters')\n",
    "axes[0].legend()\n",
    "# Plot the intertia elbow plot\n",
    "axes[1].plot(range(2, 20), inertias)\n",
    "axes[1].set_title('Inertias')\n",
    "\n",
    "# Installing a python package for\n",
    "# identifying the elbow of a line plot\n",
    "! pip install kneed -q\n",
    "from kneed import KneeLocator\n",
    "elbow = KneeLocator(range(2,20),inertias,\n",
    "                    curve='convex',\n",
    "                    direction='decreasing',\n",
    "                    interp_method='interp1d').knee\n",
    "best_cluster_inertia = elbow\n",
    "# Plotting a vertical line indicating the optimal n_clusters\n",
    "axes[1].vlines(elbow, 0, max(inertias), linestyle='--', label='Best n_clusters')\n",
    "axes[1].legend()\n",
    "\n",
    "\n",
    "# Plot the clusters using the score deemed best by silhouette score\n",
    "kmeans = KMeans(n_clusters=best_cluster).fit(unknown)\n",
    "preds = kmeans.predict(unknown)\n",
    "axes[2].scatter(unknown[:,0], unknown[:,1], c=preds)\n",
    "axes[2].set_title('Model Clusters\\nusing Silhouette Score')\n",
    "\n",
    "# Print the true number of cluster vs our clusters\n",
    "print('Number of true clusters:', n_clusters)\n",
    "print('Best n_clusters using silhouette score:', best_cluster)\n",
    "print('Best n_clusters using inertias:', best_cluster_inertia)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841ef34d-9cee-4baf-bb8f-82192c681459",
   "metadata": {
    "index": 22
   },
   "source": [
    "**Note:** This will not always work perfectly. The clusters are created randomly, and sometimes the clusters will not be obviously seperable! If you ran the code above and you ended up with a number of clusters that deviated from the true value, run  all of the code that creates this experiment a couple times, and evaluate how the arrangement of the clusters impacts the silhouette score and inertia's estimation!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4fbd1840-2730-4b6f-afc3-c9b8b2db84a6",
   "metadata": {
    "index": 23
   },
   "source": [
    "# Cluster Data 3\n",
    "\n",
    "In the cell below, we visualize the second dataset and assigned the data to the variable `data_3`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2935ec0-549a-452a-b655-f5a9fe4b0b42",
   "metadata": {
    "index": 24
   },
   "outputs": [],
   "source": [
    "# Run this cell unchanged\n",
    "\n",
    "data_3 = data[2]\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(data_3[:,0], data_3[:,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab03df6e-1b5e-4e37-ba02-6127597372bf",
   "metadata": {
    "index": 25
   },
   "source": [
    "In the cell below, fit Kmeans to `data_3` and generate predictions. Store your predictions in the variable `preds` and plot the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d84a31a-4b91-458c-8f42-8b2307a91156",
   "metadata": {
    "index": 26
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a088a96-b8f2-4f02-a88e-be53fc570264",
   "metadata": {
    "index": 28
   },
   "source": [
    "# Cluster data 4\n",
    "In the cell below, we visualize the second dataset and assigned the data to the variable `data_4`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "514b1bcf-e575-4b68-9696-bb511cfe3fc7",
   "metadata": {
    "index": 29
   },
   "outputs": [],
   "source": [
    "# Run this cell unchanged\n",
    "\n",
    "data_4 = data[3]\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.scatter(data_4[:,0], data_4[:,1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "029e1feb-aa24-4e4f-9bf8-ee23db8ce68d",
   "metadata": {
    "index": 30
   },
   "source": [
    "In the cell below, fit Kmeans to `data_4` and generate predictions. Store your predictions in the variable `preds` and plot the results!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "501fbaf9-79b8-43f3-b480-f40fdf44c62e",
   "metadata": {
    "index": 31
   },
   "outputs": [],
   "source": [
    "# Your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86af5be0-2b79-495c-acf4-1ab7a85ef17e",
   "metadata": {
    "index": 33
   },
   "source": [
    "## Apply Agglomerative Clustering to Data 4\n",
    "\n",
    "Agglomerative clustering is a form of hierarchical clustering that begins by assigning every point their own cluster, and merging clusters together until the desired number of clusters are reached.\n",
    "\n",
    "A very significant hyperparameter for Agglomerative clustering is `linkage`.\n",
    "\n",
    "**Here are the linkage options available in sklearn:**\n",
    "\n",
    "`'ward'` minimizes the variance of the clusters being merged.\n",
    "\n",
    "`'average'` uses the average of the distances of each observation of the two sets.\n",
    "\n",
    "`'complete'` or ‘maximum’ linkage uses the maximum distances between all observations of the two sets.\n",
    "\n",
    "`'single'` uses the minimum of the distances between all observations of the two sets.\n",
    "\n",
    "-----\n",
    "\n",
    "`ward` is the default linkage for sklearn's Agglomerative clustering. Let's see how it does."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6399f0-e71e-4f19-9df9-9cd5b2003fa7",
   "metadata": {
    "index": 34,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Run this cell unchanged\n",
    "\n",
    "# Import AgglomerativeClustering\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Initialize ward clustering\n",
    "ward = AgglomerativeClustering(n_clusters=2, linkage='ward')\n",
    "# Fit the clustering model\n",
    "ward.fit(data_4)\n",
    "# Collect the predicted labels\n",
    "preds = ward.labels_\n",
    "# Plot the results\n",
    "plt.scatter(data_4[:,0], data_4[:,1], c=preds);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5396c9af-0cf8-41a9-bf7c-820d0491aeff",
   "metadata": {
    "index": 35
   },
   "source": [
    "Not great! For this data, the centroid of a cluster and the variance of a cluster are not particularly informative of the cluster's boundaries. (The centers for both clusters are essentially the same!) Because ward is attempting to minimize the variance within clusters, which is a calculation that heavily weights the centroid, ward proves to be a poor choice. Let's plot the centroids to make this a little more tangible. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a918f02f-43c5-48ed-9bf8-b6fe28a8b4d7",
   "metadata": {
    "index": 36
   },
   "outputs": [],
   "source": [
    "# Run this cell unchanged\n",
    "\n",
    "# Create dataframe to make filtering simpler\n",
    "import pandas as pd\n",
    "ward = pd.DataFrame(data_4).assign(label=preds)\n",
    "\n",
    "# Filter dataframe according to their clusters\n",
    "\n",
    "label_0_center = ward.query('label==0').mean()[:2] # Calculate the mean to \n",
    "label_1_center = ward.query('label==1').mean()[:2] # get the coordinates of the centroid\n",
    "\n",
    "# Plot the results\n",
    "plt.scatter(data_4[:,0], data_4[:,1], c=preds);\n",
    "plt.scatter(label_0_center[0], label_0_center[1], s=100, label='Cluster 0 Centroid')\n",
    "plt.scatter(label_1_center[0], label_1_center[1], s=100, label='Cluster 1 Centroid')\n",
    "plt.legend(loc='upper left');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c01a74bf-14bf-41a5-88d5-c824a9608c09",
   "metadata": {
    "index": 37
   },
   "source": [
    "The results of ward clustering with this data are not that much different from the results produced by KMeans. Kmeans and Ward clustering are both methods that focus on minimizing variance and because of this, the structures that Kmeans and Ward fail to understand are quite similar. \n",
    "\n",
    "But if we apply ward clustering to the previous dataset we will see a major stength of Ward over Kmeans."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d657400-99b4-498e-870e-8825472d7a5d",
   "metadata": {
    "index": 38
   },
   "outputs": [],
   "source": [
    "# Run this cell unchanged\n",
    "\n",
    "ward = AgglomerativeClustering(n_clusters=3, linkage='ward')\n",
    "ward.fit(data_3)\n",
    "preds = ward.labels_\n",
    "plt.scatter(data_3[:,0], data_3[:,1], c=preds);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c61972bb-689d-4309-ba13-d1898a104880",
   "metadata": {
    "index": 39
   },
   "source": [
    "Above we can see that AgglomerativeClustering is much better at finding clusters when the variance of the clusters are not consistent. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee5c6d6d-a5f5-4f73-bea5-97be8f6e0288",
   "metadata": {
    "index": 40
   },
   "source": [
    "## Bonus\n",
    "\n",
    "When it comes to the fourth dataset, there are several options for modeling the correct clusters. The first one is to use the linkage setting `'single'` with Agglomerative clustering. Single linkage creates clusters by finding the points from different clusters that are closest to each other and merging their clusters together. For the 4th dataset, because the two clusters never touch, and because distance between the clusters is much larger smaller than the points within the cluster, the clusters end up being quite perfect!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cfed279-acc8-4504-93b2-d4010f8f17cf",
   "metadata": {
    "index": 41
   },
   "outputs": [],
   "source": [
    "# Run this cell unchanged\n",
    "\n",
    "options = ['ward', 'complete', 'average', 'single']\n",
    "fig, axes = plt.subplots(2,2, figsize=(15,6))\n",
    "\n",
    "for idx in range(len(options)):\n",
    "    ax = axes[idx//2, idx%2]\n",
    "    option = options[idx]\n",
    "\n",
    "    agg_cluster = AgglomerativeClustering(n_clusters=2, linkage=option)\n",
    "    agg_cluster.fit(data_4)\n",
    "    preds = agg_cluster.labels_\n",
    "    ax.scatter(data_4[:,0], data_4[:,1], c=preds)\n",
    "    ax.set_title(option)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79ce81f2-8bc8-40a6-aff1-b9d982e3c77f",
   "metadata": {
    "index": 42
   },
   "source": [
    "Sklearn's recommended approach for this data is to set linkage to `average` and calculate the \"connectivity\" matrix which is just a matrix describing the similarity of each row in the dataset.\n",
    "\n",
    "Let's look at an example of this matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "446b58c6-fb12-4d4a-b6df-2145831f7952",
   "metadata": {
    "index": 43
   },
   "outputs": [],
   "source": [
    "# Run this cell unchanged\n",
    "\n",
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "# A simple dataset\n",
    "X = [[0], \n",
    "     [3], \n",
    "     [1]]\n",
    "\n",
    "# Pass the simple dataset into kneighbors_graph\n",
    "A = kneighbors_graph(X, 2, mode='connectivity', include_self=True)\n",
    "# Output the connectivity matrix\n",
    "A.toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6f55514-34bb-4c4b-af3b-7fe9f38f434f",
   "metadata": {
    "index": 44
   },
   "source": [
    "Above we end up with a matrix, where the first row is a lot like the last row and the second row is more like the last row than the first.\n",
    "\n",
    "If we consider that the original rows of our data were\n",
    "\n",
    "```\n",
    "0\n",
    "3\n",
    "1\n",
    "```\n",
    "\n",
    "Where 0 is pretty close to 1 and 3 is closer to 1 than it is to 0, the rows of our connectivity matrix are quite sensical!\n",
    "\n",
    "Let's create a connectivity matrix for data_4 and pass it into an Average Linking Cluster Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d35f0ac-86e4-447c-ad33-73ffabe27e3d",
   "metadata": {
    "index": 45
   },
   "outputs": [],
   "source": [
    "# Run this cell unchanged\n",
    "\n",
    "connectivity = kneighbors_graph(data_4, n_neighbors=10, include_self=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c113702-5b41-4794-af1d-424dd2b51a63",
   "metadata": {
    "index": 46
   },
   "outputs": [],
   "source": [
    "# Run this cell unchanged\n",
    "\n",
    "average = AgglomerativeClustering(linkage=\"average\",\n",
    "                                  n_clusters=2, \n",
    "                                  connectivity=connectivity)\n",
    "\n",
    "average.fit(data_4)\n",
    "preds = average.labels_\n",
    "plt.scatter(data_4[:,0], data_4[:,1], c=preds);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
